---
title: "Creating views"
author: "Mutahi"
date: '2022-06-25'
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
setwd("../")
source("global.R")
```

# Introduction

The `occurence` data is a 20GB file in the csv format which includes all the observations in the database.
This data was loaded into a sqlite table and inspected.
A csv of the data for just the nation of Poland was also analysed in R as a sample from the whole dataset.
Several important facts about the data were noted which will have an impact on both the visualisation and performance:

-   Each record in the dataset includes a count variable, so it is not the case that each record is a single observation
-   The columns for ID, occurenceID, catalogNumber and reference make up more than half of the data size, with the first three being highly redundant in their nature (all basically use the ID).
-   Many of the columns are superfluous and only a handful of key columns are required. The information we really need is:
    -   Date
    -   Latitude
    -   Longitude
    -   Country
    -   Species Scientific Name
    -   Species Common Name
    -   Count of Individuals
    -   (Information about biological classification, e.g. plant or animal, may be useful later on and can be added in via a join, together with multimedia information on each species)
-   All the columns are saved as characters, which is an storage expensive way to store information.

This notebook attempts to use the Polish data to perform basic data cleaning and define the views and data validation that will be required into the data to provide meaningful visualisations efficiently.
It focuses on the occurence data.

# Data Processing

```{r}
df <- readr::read_csv("../data/poland_occurence.csv")
```

## Creating a main table

Our goal is to keep only the data we need, in an efficient form, to visualise and score biodiversity geospatially.
We have to

-   choose the right columns
-   convert to the right types of data
-   transform the latitudes and longitudes to bin observations together within a fixed radius
-   sum up the counts so that we have each row of our table showing the count of all individuals of a given species in a particular area of a country on a particular day - each record's count value should be the actual total count given all the rest of the information

We will then write this as a SQL statement to create the main view (or possibly even save is as a table.)

We choose the most important columns required, as well as some extra columns which are necessary for further cleaning or validation.

```{r}
main_columns <- c("eventDate", "longitudeDecimal", "latitudeDecimal", "country", # Geospatial
                  "individualCount", "scientificName", "vernacularName")          # the Observations

extra_columns <- c("coordinateUncertaintyInMeters")

df_selected <- df |> 
  select(all_of(c(main_columns, extra_columns)))
```

Now we need to allocate the correct data types.
R correctly infers these in general, but let us be explicit because we need to know this for writing our view.
Let us also rename our columns while we're at it.

```{r}
df_transformed <- df_selected |> 
  transmute(
    date = as_date(eventDate),
    longitude = as.double(longitudeDecimal),
    latitude = as.double(latitudeDecimal),
    country = as.character(country),
    count = as.integer(individualCount),
    scientific_name = as.character(scientificName),
    common_name = as.character(vernacularName),
    coordinateUncertaintyInMeters = coordinateUncertaintyInMeters
  )
```

Next, to bin the observations.

One degree of latitude defines a length of about 100km.
One degree of longitude defines a varying length depending on the latitude, but it is also about 100km (at the equator).
See [here for details](https://earthscience.stackexchange.com/questions/6843/how-much-distance-does-0-001-circ-in-latitude-or-longitude-represent#:~:text=So%2C%20for%20latitude%20the%20number,football%20field%20plus%20both%20endzones.).

If we were to round our latitudes and longitudes to the nearest tenth, we would be defining an area of about 10km by 100km.
Let us try to round our latitudes and longitudes so that we get an area of about 50km by 50km.

If 1 latitude degree gives us 100km, then 1/10th of one degree (one decimal degree) gives us 10km.
So we want 5/10ths of a decimal degree.
We want to round our latitude to the nearest half degree like this

```{r}
test_vec <- c(52.21380, 53.36764, 53.38855)
round(test_vec*2/1,0)*1/2
```

The same applies for longitude

Or, as a function

```{r}
round_to_the_n_over_d <- function(x, n, d){
  round(x*d/n,0)*n/d
}
```

We can bin our data and then plot it on a map to make sure we are in the same place.

```{r}
df_binned <- df_transformed |> 
  mutate(
    latitude = round_to_the_n_over_d(latitude, 5, 10),
    longitude = round_to_the_n_over_d(longitude, 5, 10)
  )
```

We can check how points move by choosing a sample of the longitudes and latitudes and putting them on a map with leaflet.

```{r}
set.seed(134)
original_points <- df_transformed |> 
  arrange(date) |> 
  slice_sample(n = 5) |> 
  select(lng = longitude, lat = latitude)

set.seed(134)
new_points <- df_binned |> 
  arrange(date) |> 
  slice_sample(n = 5) |> 
  select(lng = longitude, lat = latitude)

bind_cols(original_points, new_points) # the rounding looks corrects

leaflet() |> 
  addTiles() |> 
  addCircleMarkers(lng = ~lng, lat = ~lat, data = original_points, color = "blue", stroke = FALSE,fillOpacity = 0.4) |> 
  addCircleMarkers(lng = ~lng, lat = ~lat, data = new_points, color = "red", stroke = FALSE,fillOpacity = 0.4)
```

The example shows that the binning is not too bad.
Interestingly in this example it moved the points over a border.
In some sense this is not desirable, but in others it is.
The actual ecosystems cross the borders, it isn't totally incorrect to cross the border.
A research team trying to understand biological diversity in a border region would probably get permits to study on both sides of the border.
For the global data, we need not even invoke national boundaries at all, so I will classify this rounding as a success.

Now we just have to aggregate our binned data.

```{r}
df_aggregated <- df_binned |> 
  select(-coordinateUncertaintyInMeters) |> 
  group_by(date, longitude, latitude, country, scientific_name, common_name) |> 
  summarise(count = sum(count), .groups = "drop") |> 
  ungroup()
```

There should be as many individuals as there were before:

```{r}
sum(df_aggregated$count) == sum(df_binned$count)
```

So now we can summarise our workflow for doing initial processing and try to convert that to sql with dbplyr:

```{r}
raw_data <- readr::read_csv("../data/poland_occurence.csv")

processed_data <- raw_data |> 
  # select columns
  select("eventDate", "longitudeDecimal", "latitudeDecimal", "country", "individualCount", "scientificName", "vernacularName") |> 
  
  # cast/transform rows, could technically be part - would it be more efficient to just have this step without the select?
  transmute(
    date = as_date(eventDate),
    longitude = as.double(longitudeDecimal),
    latitude = as.double(latitudeDecimal),
    country = as.character(country),
    count = as.integer(individualCount),
    scientific_name = as.character(scientificName),
    common_name = as.character(vernacularName)
  ) |> 
  
  # bin the points by rounding
  mutate(
    latitude = round(latitude*5/10,0)*10/5,
    longitude = round(longitude*5/10,0)*10/5
  ) |> 

  # and aggregate
  group_by(date, longitude, latitude, country, scientific_name, common_name) |> 
  summarise(count = sum(count)) |> 
  ungroup()

```

We can compare the ratio of the object sizes to see how much better this is from a performance point of view.

```{r}
as.numeric(object.size(raw_data))/as.numeric(object.size(processed_data))
```

We are working with something 1/20th the size.

Let us try to get the SQL query from dbplyr, because it might be easier than trying to write it ourselves.

I've done this in a very naive way because I don't know how smart dbplyr is and I don't want it to struggle.

```{r}
con <- DBI::dbConnect(RSQLite::SQLite(),"../data/appsilon")

raw_db_data <- tbl(con, "occurence")

raw_db_data <- raw_db_data |> 
  filter(country == "Poland")

processed_db_data <- raw_db_data |> 
  # select columns
  select(
    date            = eventDate,
    longitude       = longitudeDecimal,
    latitude        = latitudeDecimal,
    country         = country,
    count           = individualCount,
    scientific_name = scientificName,
    common_name     = vernacularName
  ) |> 
  # cast/transform rows, could technically be part - would it be more efficient to just have this step without the select?
  mutate(
    date = as_date(date),
    longitude = as.double(longitude),
    latitude = as.double(latitude),
    country = as.character(country),
    count = as.integer(count),
    scientific_name = as.character(scientific_name),
    common_name = as.character(common_name)
  ) |> 
  
  # bin the points by rounding
  mutate(
    latitude = round(latitude*10/5,0)*5/10,
    longitude = round(longitude*10/5,0)*5/10
  ) |> 

  # and aggregate
  group_by(date, longitude, latitude, country, scientific_name, common_name) |> 
  summarise(count = sum(count)) |> 
  ungroup()

dbDisconnect(con)
```

```{r}
show_query(processed_db_data)
```

Which is very ugly and probably non-optimal SQL.
But it is good because it is the result of a workflow we trust.

Running the query on my computer took about 45 seconds.

I had to make the following corrections to the sql:

-   the CAST to date didn't work, I replaced the line with using the date() function: date(`date`) AS `date`,

We will have to write this data as a table, rather than as a view.

The final table has about 60% of the data.

Finally, after making the table and exporting, I get a csv for the biodiversity_poland data (i.e. the processed data) of size 2MB.

------------------------------------------------------------------------

# Corrections

-   I realised later that we shouldn't uses the common names in this table because they are irregular, and prevent efficient aggregation.
    This has been removed in the final SQL code.

-   I was wrong in my calculation for the meters from latitude and longitude.
    You should treat one degree of both as being roughly about 111m (ignoring how it varies for longitude).
    <https://earthscience.stackexchange.com/questions/6843/how-much-distance-does-0-001-circ-in-latitude-or-longitude-represent#:~:text=So%2C%20for%20latitude%20the%20number,football%20field%20plus%20both%20endzones.>
